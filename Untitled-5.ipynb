{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d676f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_STAGE1 = 32\n",
    "BATCH_STAGE2 = 16\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    \"oxford_flowers102\",\n",
    "    split=[\"train\", \"validation\", \"test\"],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "print(f\"Classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_preprocess(image):\n",
    "    if isinstance(image, tf.Tensor):\n",
    "        image = image.numpy()\n",
    "\n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[2] == 4:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    image = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])\n",
    "    image = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "def tf_preprocess(image, label):\n",
    "    image = tf.py_function(advanced_preprocess, [image], tf.uint8)\n",
    "    image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8248236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def resnet_preprocess(image, label):\n",
    "    return preprocess_input(image), label\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2)\n",
    "])\n",
    "\n",
    "train_ds_stage1 = (ds_train\n",
    "    .map(tf_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .cache()\n",
    "    .map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "    .map(resnet_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_STAGE1)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "train_ds_stage2 = (ds_train\n",
    "    .map(tf_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .cache()\n",
    "    .map(resnet_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_STAGE2)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (ds_val\n",
    "    .map(tf_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .cache()\n",
    "    .map(resnet_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_STAGE2)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = (ds_test\n",
    "    .map(tf_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .cache()\n",
    "    .map(resnet_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_STAGE2)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model = ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "    pooling='avg'\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.Dense(512, activation='swish')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nStage 1: Training classifier head\")\n",
    "history1 = model.fit(\n",
    "    train_ds_stage1,\n",
    "    validation_data=val_ds,\n",
    "    epochs=40,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nStage 2: Fine-tuning top ResNet layers\")\n",
    "base_model.trainable = True\n",
    "\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_ds_stage2,\n",
    "    validation_data=val_ds,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f\"Final Test Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "model.save('flower_classifier_optimized.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc1 = history1.history['accuracy']\n",
    "val_acc1 = history1.history['val_accuracy']\n",
    "\n",
    "acc2 = history2.history['accuracy']\n",
    "val_acc2 = history2.history['val_accuracy']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(acc1, label='Train Accuracy (Stage 1)')\n",
    "plt.plot(val_acc1, label='Val Accuracy (Stage 1)')\n",
    "plt.plot(range(len(acc1), len(acc1)+len(acc2)), acc2, label='Train Accuracy (Stage 2)')\n",
    "plt.plot(range(len(val_acc1), len(val_acc1)+len(val_acc2)), val_acc2, label='Val Accuracy (Stage 2)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "loss1 = history1.history['loss']\n",
    "val_loss1 = history1.history['val_loss']\n",
    "\n",
    "loss2 = history2.history['loss']\n",
    "val_loss2 = history2.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss1, label='Train Loss (Stage 1)')\n",
    "plt.plot(val_loss1, label='Val Loss (Stage 1)')\n",
    "plt.plot(range(len(loss1), len(loss1)+len(loss2)), loss2, label='Train Loss (Stage 2)')\n",
    "plt.plot(range(len(val_loss1), len(val_loss1)+len(val_loss2)), val_loss2, label='Val Loss (Stage 2)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for images, labels in test_ds.take(1):\n",
    "    preds = model.predict(images)\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow((images[i] + 1) / 2)  # undo ResNet normalization\n",
    "        plt.title(f\"Pred: {np.argmax(preds[i])}\")\n",
    "        plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "methods = ['Reference Paper', 'Our Model']\n",
    "accuracy = [70, 85]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(methods, accuracy)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74124e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def predict_image(model, image_path, image_size=224):\n",
    "    # 1- Acquisition\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found or path incorrect.\")\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 2- Enhancement (CLAHE + Sharpen)\n",
    "    lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    l_clahe = clahe.apply(l)\n",
    "\n",
    "    lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "    enhanced = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "    kernel_sharpen = np.array([[-1,-1,-1],\n",
    "                               [-1, 9,-1],\n",
    "                               [-1,-1,-1]])\n",
    "    enhanced = cv2.filter2D(enhanced, -1, kernel_sharpen)\n",
    "\n",
    "    # 3- Segmentation \n",
    "    gray = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)\n",
    "    _, segmented_image = cv2.threshold(\n",
    "        gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU\n",
    "    )\n",
    "\n",
    "    # Show Segmentation\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(segmented_image, cmap=\"gray\")\n",
    "    plt.title(\"Segmentation (NOT used for prediction)\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # 4- Resize for model\n",
    "    img_resized = cv2.resize(enhanced, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img_resized)\n",
    "    plt.title(\"Enhanced Image USED for Model\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    img_tf = advanced_preprocess(img_resized)\n",
    "    img_tf = tf.expand_dims(img_tf, axis=0)       \n",
    "    img_tf = tf.cast(img_tf, tf.float32)\n",
    "    img_tf = preprocess_input(img_tf)              \n",
    "\n",
    "    predictions = model.predict(img_tf)[0]\n",
    "    class_id = np.argmax(predictions)\n",
    "    return class_id, predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cfa72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\R\\Downloads\\Sunflower_sky_backdrop.jpg\"\n",
    "\n",
    "ds_info = tfds.builder(\"oxford_flowers102\").info\n",
    "label_names = ds_info.features[\"label\"].names\n",
    "\n",
    "class_id, probabilities = predict_image(model, image_path)\n",
    "\n",
    "print(\"Predicted flower:\", label_names[class_id]) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
